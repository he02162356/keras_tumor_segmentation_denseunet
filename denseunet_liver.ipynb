{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential ,Model\n",
    "from keras.callbacks import ModelCheckpoint, History\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Activation,MaxPooling2D,UpSampling2D,Dense,BatchNormalization,Input,Reshape,multiply,add,Dropout,AveragePooling2D,GlobalAveragePooling2D,concatenate\n",
    "from keras.layers.convolutional import Conv2D,Conv2DTranspose\n",
    "from keras.regularizers import l2\n",
    "from keras.engine import Layer,InputSpec\n",
    "from keras.utils import conv_utils\n",
    "from layers import BN_ReLU_Conv, TransitionDown, TransitionUp, SoftmaxLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src_url = './dataset_liver/train_img'\n",
    "train_mask_url = './dataset_liver/train_mask'\n",
    "\n",
    "val_src_url = './dataset_liver/val_img'\n",
    "val_mask_url = './dataset_liver/val_mask'\n",
    "\n",
    "test_src_url = './dataset_liver/test_img'\n",
    "test_mask_url = './dataset_liver/test_mask'\n",
    "\n",
    "NO_OF_TRAINING_IMAGES = len(os.listdir('./dataset_liver/train_img/train'))\n",
    "NO_OF_VAL_IMAGES = len(os.listdir('./dataset_liver/val_img/val'))\n",
    "NO_OF_TEST_IMAGES = len(os.listdir('./dataset_liver/test_img/test'))\n",
    "\n",
    "\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 2\n",
    "img_h = 256\n",
    "img_w = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(  \n",
    "        rescale=1.0 / 255.0,\n",
    "        rotation_range=20,\n",
    "        #width_shift_range=0.2,\n",
    "        #height_shift_range=0.2,\n",
    "    )\n",
    "\n",
    "#train processing\n",
    "train_generator = train_datagen.flow_from_directory(train_src_url,\n",
    "                                                        target_size=(img_h, img_w),\n",
    "                                                        batch_size=BATCH_SIZE,\n",
    "                                                        color_mode='grayscale',\n",
    "                                                        seed=123,\n",
    "                                                        shuffle=True,\n",
    "                                                        class_mode=None)\n",
    "    \n",
    "train_mask_generator = train_datagen.flow_from_directory(train_mask_url,\n",
    "                                                        target_size=(img_h, img_w),\n",
    "                                                        batch_size=BATCH_SIZE,\n",
    "                                                        color_mode='grayscale',\n",
    "                                                        seed=123,\n",
    "                                                        shuffle=True,\n",
    "                                                        class_mode=None)\n",
    "\n",
    "#val processing\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1.0 / 255.0,\n",
    "    )\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(val_src_url,\n",
    "                                                        target_size=(img_h, img_w),\n",
    "                                                        batch_size=BATCH_SIZE,\n",
    "                                                        color_mode='grayscale',\n",
    "                                                        seed=1,\n",
    "                                                        shuffle=True,\n",
    "                                                        class_mode=None)\n",
    "    \n",
    "val_mask_generator = train_datagen.flow_from_directory(val_mask_url,\n",
    "                                                        target_size=(img_h, img_w),\n",
    "                                                        batch_size=BATCH_SIZE,\n",
    "                                                        color_mode='grayscale',\n",
    "                                                        seed=1,\n",
    "                                                        shuffle=True,\n",
    "                                                        class_mode=None)\n",
    "#test processing\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1.0 / 255.0,\n",
    "    )\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_src_url,\n",
    "                                                        target_size=(img_h, img_w),\n",
    "                                                        batch_size=BATCH_SIZE,\n",
    "                                                        color_mode='grayscale',\n",
    "                                                        class_mode=None)\n",
    "    \n",
    "test_mask_generator = test_datagen.flow_from_directory(test_mask_url,\n",
    "                                                        target_size=(img_h, img_w),\n",
    "                                                        batch_size=BATCH_SIZE,  \n",
    "                                                        color_mode='grayscale',\n",
    "                                                        class_mode=None)\n",
    "    \n",
    "    \n",
    "train_generator = zip(train_generator, train_mask_generator)\n",
    "val_generator = zip(val_generator, val_mask_generator)\n",
    "test_generator = zip(test_generator, test_mask_generator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/mad-Ye/FC-DenseNet-Keras\n",
    "def Tiramisu(\n",
    "        input_shape=(256,256,1),\n",
    "        n_classes = 3,\n",
    "        n_filters_first_conv = 48,\n",
    "        n_pool = 5,\n",
    "        growth_rate = 16 ,\n",
    "        n_layers_per_block = [4,5,7,10,12,15,12,10,7,5,4],\n",
    "        dropout_p = 0\n",
    "        ):\n",
    "    if type(n_layers_per_block) == list:\n",
    "            print(len(n_layers_per_block))\n",
    "    elif type(n_layers_per_block) == int:\n",
    "            n_layers_per_block = [n_layers_per_block] * (2 * n_pool + 1)\n",
    "    else:\n",
    "        raise ValueError\n",
    "        \n",
    "#####################\n",
    "# First Convolution #\n",
    "#####################        \n",
    "    inputs = Input(shape=input_shape)\n",
    "    stack = Conv2D(filters=n_filters_first_conv, kernel_size=3, padding='same', kernel_initializer='he_uniform')(inputs)\n",
    "    n_filters = n_filters_first_conv\n",
    "\n",
    "#####################\n",
    "# Downsampling path #\n",
    "#####################     \n",
    "    skip_connection_list = []\n",
    "    \n",
    "    for i in range(n_pool):\n",
    "        for j in range(n_layers_per_block[i]):\n",
    "            l = BN_ReLU_Conv(stack, growth_rate, dropout_p=dropout_p)\n",
    "            stack = concatenate([stack, l])\n",
    "            n_filters += growth_rate\n",
    "        \n",
    "        skip_connection_list.append(stack)        \n",
    "        stack = TransitionDown(stack, n_filters, dropout_p)\n",
    "    skip_connection_list = skip_connection_list[::-1]\n",
    "\n",
    "    \n",
    "#####################\n",
    "#    Bottleneck     #\n",
    "#####################     \n",
    "    block_to_upsample=[]\n",
    "    \n",
    "    for j in range(n_layers_per_block[n_pool]):\n",
    "        l = BN_ReLU_Conv(stack, growth_rate, dropout_p=dropout_p)\n",
    "        block_to_upsample.append(l)\n",
    "        stack = concatenate([stack,l])\n",
    "    block_to_upsample = concatenate(block_to_upsample)\n",
    "\n",
    "#####################\n",
    "#  Upsampling path  #\n",
    "#####################\n",
    "    for i in range(n_pool):\n",
    "        n_filters_keep = growth_rate * n_layers_per_block[n_pool + i ]\n",
    "        stack = TransitionUp(skip_connection_list[i], block_to_upsample, n_filters_keep)\n",
    "        \n",
    "        block_to_upsample = []\n",
    "        for j in range(n_layers_per_block[ n_pool + i + 1 ]):\n",
    "            l = BN_ReLU_Conv(stack, growth_rate, dropout_p=dropout_p)\n",
    "            block_to_upsample.append(l)\n",
    "            stack = concatenate([stack, l])\n",
    "        block_to_upsample = concatenate(block_to_upsample)\n",
    "\n",
    "#####################\n",
    "#  Softmax          #\n",
    "#####################\n",
    "    output = SoftmaxLayer(stack, 1)            \n",
    "    model=Model(inputs = inputs, outputs = output)    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Tiramisu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dice_loss\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    # Flatten\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dice_coef(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay \n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "lr_base = 1e-5\n",
    "lr_power = 0.995\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "    lr = (float(lr_base) ** float(lr_power)) ** float(epoch + 1)  \n",
    "    return lr\n",
    "\n",
    "lrate = LearningRateScheduler(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.compile(optimizer=Adam(lr=lr_base, beta_1=0.9, beta_2=0.999) , loss=dice_loss, metrics=[dice_coef])\n",
    "#train.compile(optimizer=Lookahead(Adam(lr_base)), loss=dice_loss, metrics=[dice_coef])\n",
    "#train.compile(optimizer=RMSprop(lr=lr_base), loss=dice_loss, metrics=[dice_coef])\n",
    "#train.compile(optimizer=RMSprop(lr=lr_base), loss=dice_p_bce, metrics=[dice_coef])\n",
    "model_checkpoint = ModelCheckpoint('denseunet_weight_liver_test.h5', monitor='val_loss', save_best_only=True)\n",
    "history =  train.fit_generator(train_generator, epochs=EPOCHS, steps_per_epoch = NO_OF_TRAINING_IMAGES//BATCH_SIZE, \n",
    "                               validation_data=val_generator, validation_steps= NO_OF_VAL_IMAGES//BATCH_SIZE, \n",
    "                               verbose=1, shuffle=True,\n",
    "                               callbacks=[model_checkpoint, lrate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot train processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,10),dpi=100)\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(history.history['dice_coef'])\n",
    "plt.plot(history.history['val_dice_coef'])\n",
    "plt.title('model acc')\n",
    "plt.ylabel('dice_coef')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'],loc = 'upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,10),dpi=100)\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'],loc = 'upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.save_weights(\"denseunet_weight_liver_final_test.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Tiramisu()\n",
    "test.load_weights(\"denseunet_weight_liver_final_test.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "import skimage.io as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll = io.ImageCollection('./seg_liver_result/*.png')\n",
    "test_scans = io.concatenate_images(coll)\n",
    "test_scans = test_scans/ 255.0\n",
    "test_scans = resize(test_scans, (test_scans.shape[0], img_h, img_w, 1))\n",
    "test_scans = np.reshape(test_scans ,(-1, img_h, img_w,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_mask_test = test.predict(test_scans, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(3):\n",
    "        print ('scan '+str(j))\n",
    "        f, ax = plt.subplots(1, 3,figsize=(10,5))\n",
    "        ax[0].imshow(test_scans[j,:,:,0],cmap=plt.cm.gray)\n",
    "        imgs_mask_test[imgs_mask_test > 0.5] = 1\n",
    "        imgs_mask_test[imgs_mask_test < 0.5] = 0\n",
    "        ax[1].imshow((imgs_mask_test[j,:,:,0] * 255.).astype(np.uint8) ,cmap=plt.cm.gray)\n",
    "        ax[2].imshow((test_scans[j,:,:,0])*(imgs_mask_test[j,:, :, 0]) ,cmap=plt.cm.gray)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "i = 0\n",
    "for mask,scans in zip(imgs_mask_test,test_scans) :\n",
    "    i+=1\n",
    "    mask[mask > 0.5] = 1\n",
    "    mask[mask < 0.5] = 0\n",
    "    image = scans[:,:,0]*mask[:,:,0]\n",
    "    pic = cv2.resize(image, (512, 512), interpolation=cv2.INTER_CUBIC)\n",
    "    io.imsave(os.path.join(r'./seg_liver_result', str(i) + '_pred.png'), pic)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.compile(optimizer=Adam(lr=1e-5, beta_1=0.9, beta_2=0.999) , loss=dice_loss, metrics=[dice_coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, loss = test.evaluate_generator(test_generator, steps=NO_OF_TEST_IMAGES//BATCH_SIZE+1 ,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
